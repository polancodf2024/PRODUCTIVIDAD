import streamlit as st
import pandas as pd
import io
from datetime import datetime
import calendar
import paramiko
import time
import os
import logging
from pathlib import Path

# Configuraci√≥n de logging
logging.basicConfig(
    filename='monitoreo.log',
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)

# ====================
# CONFIGURACI√ìN INICIAL
# ====================
class Config:
    def __init__(self):
        # Configuraci√≥n SFTP
        self.REMOTE_PRODUCTOS_FILE = "pro_productos_total.csv"  # Nombre completo del archivo remoto
        self.TIMEOUT_SECONDS = 30
        
        self.REMOTE = {
            'HOST': st.secrets["sftp"]["host"],
            'USER': st.secrets["sftp"]["user"],
            'PASSWORD': st.secrets["sftp"]["password"],
            'PORT': st.secrets["sftp"]["port"],
            'DIR': st.secrets["sftp"]["dir"]
        }

CONFIG = Config()

# ==================
# CLASE SSH MEJORADA
# ==================
class SSHManager:
    MAX_RETRIES = 3
    RETRY_DELAY = 5  # segundos

    @staticmethod
    def get_connection():
        """Establece conexi√≥n SSH segura con reintentos"""
        ssh = paramiko.SSHClient()
        ssh.set_missing_host_key_policy(paramiko.AutoAddPolicy())
        
        for attempt in range(SSHManager.MAX_RETRIES):
            try:
                ssh.connect(
                    hostname=CONFIG.REMOTE['HOST'],
                    port=CONFIG.REMOTE['PORT'],
                    username=CONFIG.REMOTE['USER'],
                    password=CONFIG.REMOTE['PASSWORD'],
                    timeout=CONFIG.TIMEOUT_SECONDS
                )
                logging.info(f"Conexi√≥n SSH establecida (intento {attempt + 1})")
                return ssh
            except Exception as e:
                logging.warning(f"Intento {attempt + 1} fallido: {str(e)}")
                if attempt < SSHManager.MAX_RETRIES - 1:
                    time.sleep(SSHManager.RETRY_DELAY)
                else:
                    logging.error("Fallo definitivo al conectar via SSH")
                    st.error(f"Error de conexi√≥n SSH despu√©s de {SSHManager.MAX_RETRIES} intentos: {str(e)}")
                    return None

    @staticmethod
    def verify_file_integrity(local_path, remote_path, sftp):
        """Verifica que el archivo se transfiri√≥ correctamente"""
        try:
            local_size = os.path.getsize(local_path)
            remote_size = sftp.stat(remote_path).st_size
            return local_size == remote_size
        except Exception as e:
            logging.error(f"Error verificando integridad: {str(e)}")
            return False

    @staticmethod
    def download_remote_file(remote_path, local_path):
        """Descarga un archivo con verificaci√≥n de integridad"""
        for attempt in range(SSHManager.MAX_RETRIES):
            ssh = SSHManager.get_connection()
            if not ssh:
                return False
                
            try:
                with ssh.open_sftp() as sftp:
                    try:
                        sftp.stat(remote_path)
                    except FileNotFoundError:
                        logging.error(f"Archivo remoto no encontrado: {remote_path}")
                        return False
                        
                    sftp.get(remote_path, local_path)
                    
                    if SSHManager.verify_file_integrity(local_path, remote_path, sftp):
                        logging.info(f"Archivo descargado correctamente: {remote_path} a {local_path}")
                        return True
                    else:
                        logging.warning(f"Error de integridad en descarga, reintentando... (intento {attempt + 1})")
                        if attempt < SSHManager.MAX_RETRIES - 1:
                            time.sleep(SSHManager.RETRY_DELAY)
                        else:
                            raise Exception("Fallo en verificaci√≥n de integridad despu√©s de m√∫ltiples intentos")
                            
            except Exception as e:
                logging.error(f"Error en descarga (intento {attempt + 1}): {str(e)}")
                if attempt == SSHManager.MAX_RETRIES - 1:
                    st.error(f"Error descargando archivo remoto despu√©s de {SSHManager.MAX_RETRIES} intentos: {str(e)}")
                    return False
                    
            finally:
                ssh.close()

def sync_productos_file():
    """Sincroniza el archivo productos_total.csv desde el servidor remoto"""
    try:
        remote_path = os.path.join(CONFIG.REMOTE['DIR'], CONFIG.REMOTE_PRODUCTOS_FILE)
        local_path = "productos_total.csv"
        
        with st.spinner("üîÑ Sincronizando archivo productos_total.csv desde el servidor..."):
            if SSHManager.download_remote_file(remote_path, local_path):
                st.success("‚úÖ Archivo productos_total.csv sincronizado correctamente")
                return True
            else:
                st.error("‚ùå No se pudo descargar el archivo productos_total.csv del servidor")
                return False
    except Exception as e:
        st.error(f"‚ùå Error en sincronizaci√≥n: {str(e)}")
        logging.error(f"Sync Error: {str(e)}")
        return False

def main():
    # A√±adir logo en la parte superior
    st.image("escudo_COLOR.jpg", width=200)

    st.title("An√°lisis de Art√≠culos")

    # Sincronizar archivo productos_total.csv al inicio
    if not sync_productos_file():
        st.warning("‚ö†Ô∏è Trabajando con copia local de productos_total.csv debido a problemas de conexi√≥n")

    # Verificar si el archivo local existe
    if not Path("productos_total.csv").exists():
        st.error("No se encontr√≥ el archivo productos_total.csv")
        return

    try:
        # Leer y procesar el archivo con los nuevos campos sni y sii
        df = pd.read_csv("productos_total.csv", header=0, encoding='utf-8')
        df.columns = df.columns.str.strip()  # Limpiar espacios en nombres de columnas

        # Verificaci√≥n de columnas (para diagn√≥stico)
        logging.info(f"Columnas detectadas: {df.columns.tolist()}")

        # Verificar que los campos clave existen
        if 'nombramiento' not in df.columns:
            st.warning("El archivo no contiene el campo 'nombramiento'")
        if 'sni' not in df.columns or 'sii' not in df.columns:
            st.warning("El archivo productos_total.csv no contiene los campos 'sni' y/o 'sii'")

        # Convertir y validar fechas
        df['pub_date'] = pd.to_datetime(df['pub_date'], errors='coerce')
        df = df[(df['estado'] == 'A') & (df['pub_date'].notna())]

        if df.empty:
            st.warning("No hay publicaciones v√°lidas para analizar")
            return

        st.success(f"Datos cargados correctamente. Registros activos: {len(df)}")

        # Obtener rangos de fechas disponibles
        min_date = df['pub_date'].min()
        max_date = df['pub_date'].max()

        # Selector de rango mes-a√±o con ayuda
        st.header("üìÖ Selecci√≥n de Periodo")
        col1, col2 = st.columns(2)

        with col1:
            start_year = st.selectbox("A√±o inicio",
                                   range(min_date.year, max_date.year+1),
                                   index=0,
                                   help="Selecciona el a√±o inicial para el an√°lisis.")
            start_month = st.selectbox("Mes inicio",
                                    range(1, 13),
                                    index=min_date.month-1,
                                    format_func=lambda x: datetime(1900, x, 1).strftime('%B'),
                                    help="Selecciona el mes inicial para el an√°lisis.")

        with col2:
            end_year = st.selectbox("A√±o t√©rmino",
                                  range(min_date.year, max_date.year+1),
                                  index=len(range(min_date.year, max_date.year+1))-1,
                                  help="Selecciona el a√±o final para el an√°lisis.")
            end_month = st.selectbox("Mes t√©rmino",
                                   range(1, 13),
                                   index=max_date.month-1,
                                   format_func=lambda x: datetime(1900, x, 1).strftime('%B'),
                                   help="Selecciona el mes final para el an√°lisis.")

        # Calcular fechas de inicio y fin
        start_day = 1
        end_day = calendar.monthrange(end_year, end_month)[1]

        date_start = datetime(start_year, start_month, start_day)
        date_end = datetime(end_year, end_month, end_day)

        # Filtrar dataframe
        filtered_df = df[(df['pub_date'] >= pd.to_datetime(date_start)) &
                       (df['pub_date'] <= pd.to_datetime(date_end))]

        # Obtener art√≠culos √∫nicos para estad√≠sticas precisas
        unique_articles = filtered_df.drop_duplicates(subset=['article_title'])

        st.markdown(f"**Periodo seleccionado:** {date_start.strftime('%d/%m/%Y')} - {date_end.strftime('%d/%m/%Y')}",
                   help="Rango de fechas seleccionado para el an√°lisis.")
        st.markdown(f"**Art√≠culos encontrados:** {len(filtered_df)}",
                   help="Total de registros en el periodo, incluyendo posibles duplicados del mismo art√≠culo.")
        st.markdown(f"**Art√≠culos √∫nicos:** {len(unique_articles)}",
                   help="Cantidad de art√≠culos cient√≠ficos distintos, eliminando duplicados.")

        duplicates_count = len(filtered_df) - len(unique_articles)

        if duplicates_count > 0:
            if len(unique_articles) == 1:
                st.warning(f"‚ö†Ô∏è **Nota:** Se detect√≥ {duplicates_count} art√≠culo duplicado.")
            else:
                st.warning(f"‚ö†Ô∏è **Nota:** Se detectaron {duplicates_count} art√≠culos duplicados.")

        if filtered_df.empty:
            st.warning("No hay publicaciones en el periodo seleccionado")
            return

        # An√°lisis consolidado en tablas
        st.header("üìä Estad√≠sticas Consolidadas",
                help="M√©tricas generales basadas en los filtros aplicados.")

        # Tabla 1: Productividad por investigador (ART√çCULOS √öNICOS) con participaci√≥n
        st.subheader("üîç Productividad por investigador",
                   help="Muestra cu√°ntos art√≠culos √∫nicos tiene cada investigador y su posici√≥n de autor√≠a.")

        investigator_stats = filtered_df.groupby('investigator_name').agg(
            Articulos_Unicos=('article_title', lambda x: len(set(x))),
            Participaciones=('participation_key', lambda x: ', '.join(sorted(set(x))))
        ).reset_index()

        investigator_stats = investigator_stats.sort_values('Articulos_Unicos', ascending=False)
        investigator_stats.columns = ['Investigador', 'Art√≠culos √∫nicos', 'Posici√≥n de autor√≠a']

        # A√±adir fila de totales
        total_row = pd.DataFrame({
            'Investigador': ['TOTAL'],
            'Art√≠culos √∫nicos': [investigator_stats['Art√≠culos √∫nicos'].sum()],
            'Posici√≥n de autor√≠a': ['']
        })
        investigator_stats = pd.concat([investigator_stats.head(10), total_row], ignore_index=True)

        # Mostrar tabla con enlaces clickeables - VERSI√ìN MODIFICADA
        for index, row in investigator_stats.iterrows():
            if row['Investigador'] != 'TOTAL':
                with st.expander(f"{row['Investigador']} - {row['Art√≠culos √∫nicos']} art√≠culos"):
                    investigator_articles = filtered_df[filtered_df['investigator_name'] == row['Investigador']]
                    unique_articles_investigator = investigator_articles.drop_duplicates(subset=['article_title'])

                    # Mostrar todos los campos disponibles excepto columnas internas
                    excluded_columns = ['estado']  # Solo excluir columna estado
                    display_columns = [col for col in unique_articles_investigator.columns
                                     if col not in excluded_columns]

                    # Configuraci√≥n especial para columnas
                    column_config = {
                        "pub_date": st.column_config.DateColumn("Fecha publicaci√≥n", format="DD/MM/YYYY"),
                        "doi": st.column_config.LinkColumn("DOI"),
                        "pmid": st.column_config.NumberColumn("PMID"),
                        "participation_key": st.column_config.TextColumn("Participaci√≥n"),
                        "economic_number": st.column_config.TextColumn("N√∫mero econ√≥mico"),
                        "nombramiento": st.column_config.TextColumn("Nombramiento"),
                        "sni": st.column_config.TextColumn("SNI"),
                        "sii": st.column_config.TextColumn("SII")
                    }

                    st.dataframe(
                        unique_articles_investigator[display_columns],
                        column_config=column_config,
                        use_container_width=True,
                        hide_index=True
                    )

                    # Bot√≥n de descarga
                    csv = unique_articles_investigator.to_csv(index=False).encode('utf-8')
                    st.download_button(
                        label="Descargar producci√≥n cient√≠fica completa",
                        data=csv,
                        file_name=f"produccion_{row['Investigador'].replace(' ', '_')}.csv",
                        mime='text/csv',
                        key=f"download_{index}"
                    )

        # Tabla 2: Revistas m√°s publicadas
        st.subheader("üìö Revistas m√°s utilizadas",
                   help="Listado de revistas cient√≠ficas ordenadas por cantidad de art√≠culos publicados.")
        journal_stats = unique_articles.groupby('journal_full').agg(
            Total_Articulos=('journal_full', 'size'),
            Grupo_JCR=('jcr_group', lambda x: x.mode()[0] if not x.mode().empty else 'No disponible')
        ).reset_index()
        journal_stats = journal_stats.sort_values('Total_Articulos', ascending=False)
        journal_stats.columns = ['Revista', 'Art√≠culos √∫nicos', 'Grupo JCR m√°s frecuente']

        total_row = pd.DataFrame({
            'Revista': ['TOTAL'],
            'Art√≠culos √∫nicos': [journal_stats['Art√≠culos √∫nicos'].sum()],
            'Grupo JCR m√°s frecuente': ['']
        })
        journal_stats = pd.concat([journal_stats.head(10), total_row], ignore_index=True)
        st.dataframe(journal_stats, hide_index=True)

        # Tabla 3: Disciplinas m√°s comunes
        st.subheader("üß™ L√≠neas de investigaci√≥n m√°s frecuentes",
                   help="L√≠neas de investigaci√≥n m√°s utilizadas en los art√≠culos.")
        try:
            all_keywords = []
            for keywords in unique_articles['selected_keywords']:
                cleaned = str(keywords).strip("[]'").replace("'", "").split(", ")
                all_keywords.extend([k.strip() for k in cleaned if k.strip()])

            keyword_stats = pd.Series(all_keywords).value_counts().reset_index()
            keyword_stats.columns = ['Disciplina', 'Frecuencia']

            total_row = pd.DataFrame({
                'Disciplina': ['TOTAL'],
                'Frecuencia': [keyword_stats['Frecuencia'].sum()]
            })
            keyword_stats = pd.concat([keyword_stats.head(10), total_row], ignore_index=True)
            st.dataframe(keyword_stats, hide_index=True)
        except:
            st.warning("No se pudieron procesar las disciplinas")

        # Tabla 4: Distribuci√≥n por grupos JCR
        st.subheader("üèÜ Distribuci√≥n por √≠ndice de impacto",
                   help="Clasificaci√≥n de art√≠culos seg√∫n el factor de impacto de las revistas.")
        jcr_stats = unique_articles['jcr_group'].value_counts().reset_index()
        jcr_stats.columns = ['Grupo JCR', 'Art√≠culos √∫nicos']

        total_row = pd.DataFrame({
            'Grupo JCR': ['TOTAL'],
            'Art√≠culos √∫nicos': [jcr_stats['Art√≠culos √∫nicos'].sum()]
        })
        jcr_stats = pd.concat([jcr_stats, total_row], ignore_index=True)
        st.dataframe(jcr_stats, hide_index=True)

        # Tabla 5: Distribuci√≥n temporal
        st.subheader("üï∞Ô∏è Distribuci√≥n mensual de art√≠culos",
                    help="Evoluci√≥n mensual de la producci√≥n cient√≠fica.")
        time_stats = unique_articles['pub_date'].dt.strftime('%Y-%m').value_counts().sort_index().reset_index()
        time_stats.columns = ['Mes-A√±o', 'Art√≠culos √∫nicos']

        total_row = pd.DataFrame({
            'Mes-A√±o': ['TOTAL'],
            'Art√≠culos √∫nicos': [time_stats['Art√≠culos √∫nicos'].sum()]
        })
        time_stats = pd.concat([time_stats, total_row], ignore_index=True)
        st.dataframe(time_stats, hide_index=True)

        # Tabla 6: Distribuci√≥n por nivel SNI
        if 'sni' in unique_articles.columns:
            st.subheader("üìä Distribuci√≥n por nivel SNI",
                        help="Clasificaci√≥n seg√∫n el nivel del Sistema Nacional de Investigadores.")
            sni_stats = unique_articles['sni'].value_counts().reset_index()
            sni_stats.columns = ['Nivel SNI', 'Art√≠culos √∫nicos']

            total_row = pd.DataFrame({
                'Nivel SNI': ['TOTAL'],
                'Art√≠culos √∫nicos': [sni_stats['Art√≠culos √∫nicos'].sum()]
            })
            sni_stats = pd.concat([sni_stats, total_row], ignore_index=True)
            st.dataframe(sni_stats, hide_index=True)
        else:
            st.warning("El campo 'sni' no est√° disponible en los datos")

        # Tabla 7: Distribuci√≥n por nivel SII
        if 'sii' in unique_articles.columns:
            st.subheader("üìà Distribuci√≥n por nivel SII",
                        help="Clasificaci√≥n seg√∫n el nivel del Sistema Institucional de Investigaci√≥n.")
            sii_stats = unique_articles['sii'].value_counts().reset_index()
            sii_stats.columns = ['Nivel SII', 'Art√≠culos √∫nicos']

            total_row = pd.DataFrame({
                'Nivel SII': ['TOTAL'],
                'Art√≠culos √∫nicos': [sii_stats['Art√≠culos √∫nicos'].sum()]
            })
            sii_stats = pd.concat([sii_stats, total_row], ignore_index=True)
            st.dataframe(sii_stats, hide_index=True)
        else:
            st.warning("El campo 'sii' no est√° disponible en los datos")

        # Tabla 8: Distribuci√≥n por tipo de nombramiento
        if 'nombramiento' in unique_articles.columns:
            st.subheader("üëî Distribuci√≥n por nombramientos",
                        help="Clasificaci√≥n seg√∫n el nombramiento de los autores.")
            nombramiento_stats = unique_articles['nombramiento'].value_counts().reset_index()
            nombramiento_stats.columns = ['Tipo de Nombramiento', 'Art√≠culos √∫nicos']

            total_row = pd.DataFrame({
                'Tipo de Nombramiento': ['TOTAL'],
                'Art√≠culos √∫nicos': [nombramiento_stats['Art√≠culos √∫nicos'].sum()]
            })
            nombramiento_stats = pd.concat([nombramiento_stats, total_row], ignore_index=True)
            st.dataframe(nombramiento_stats, hide_index=True)
        else:
            st.warning("El campo 'nombramiento' no est√° disponible en los datos")

        # Descargar archivo completo
        st.header("üì• Descargar Datos Completos")
        if Path("productos_total.csv").exists():
            with open("productos_total.csv", "rb") as file:
                btn = st.download_button(
                    label="Descargar archivo pro_productos_total.csv completo",
                    data=file,
                    file_name="pro_productos_total.csv",
                    mime="text/csv",
                    help="Descarga el archivo CSV completo con todos los datos"
                )
            if btn:
                st.success("Descarga iniciada")
        else:
            st.warning("El archivo productos_total.csv no est√° disponible para descargar")

    except Exception as e:
        st.error(f"Error al procesar el archivo: {str(e)}")
        logging.error(f"Error en main: {str(e)}")

if __name__ == "__main__":
    main()

